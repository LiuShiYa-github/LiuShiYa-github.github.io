

<!DOCTYPE html>
<html lang="zh-CN">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>ceph+docker方式搭建owncloud实现私网云盘 - LiuShiYa-github</title>
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="google" content="notranslate" />

  
  <meta name="keywords" content="zhaoo, hexo-theme-zhaoo,"> 
  
  <meta name="description" content="北冥有鱼，其名为鲲，鲲之大，不知其几千里也,ceph+docker方式搭建owncloud实现私网..."> 
  
  <meta name="author" content="刘世亚"> 

  
    <link rel="icon" href="/images/icons/favicon-16x16.png" type="image/png" sizes="16x16">
  
  
    <link rel="icon" href="/images/icons/favicon-32x32.png" type="image/png" sizes="32x32">
  
  
    <link rel="apple-touch-icon" href="/images/icons/apple-touch-icon.png" sizes="180x180">
  
  
    <meta rel="mask-icon" href="/images/icons/stun-logo.svg" color="#333333">
  
  
    <meta rel="msapplication-TileImage" content="/images/icons/favicon-144x144.png">
    <meta rel="msapplication-TileColor" content="#000000">
  

  
<link rel="stylesheet" href="/css/style.css">


  
  
<link rel="stylesheet" href="//at.alicdn.com/t/font_1445822_58xq2j9v1id.css">

  

  
  
  
<link rel="stylesheet" href="https://cdn.bootcss.com/fancybox/3.5.7/jquery.fancybox.min.css">

  

  
  
  
<link rel="stylesheet" href="https://cdn.bootcss.com/highlight.js/9.18.1/styles/xcode.min.css">

  

  
  
  
<link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css">

  

  <script>
    var CONFIG = window.CONFIG || {}
    CONFIG = {
      fancybox: true,
      pjax: true,
      lazyload: {
        enable: true,
        loadingImage: '/images/theme/loading.gif',
      },
      donate_alipay: 'https://pic.izhaoo.com/alipay.jpg',
      donate_wechat: 'https://pic.izhaoo.com/wechat.jpg',
      gitalk: {
        enable: false,
        clientID: '',
        clientSecret: '',
        id: window.location.pathname,
        repo: '',
        owner: '',
        admin: ''
      },
      motto: {
        api: '',
        default: '寂寞的时候，我会卷曲着自己，假装被拥抱着。'
      },
      galleries: {
        enable: 'true'
      },
      fab: {
        enable: 'true',
        alwaysShow: 'false'
      }
    }
  </script>

  

  
<meta name="generator" content="Hexo 4.2.0"></head>
<body class="lock-screen">
  <div class="loading"></div>
  <nav class="menu">
  <div class="menu-close">
    <i class="iconfont iconplus"></i>
  </div>
  <ul class="menu-content">
    
    
    
    
    <li class="menu-item"><a href="/ "> 首页</a></li>
    
    
    
    
    <li class="menu-item"><a href="/galleries "> 摄影</a></li>
    
    
    
    
    <li class="menu-item"><a href="/archives "> 归档</a></li>
    
    
    
    
    <li class="menu-item"><a href="/tags "> 标签</a></li>
    
    
    
    
    <li class="menu-item"><a href="/categories "> 分类</a></li>
    
    
    
    
    <li class="menu-item"><a href="/about "> 关于</a></li>
    
  </ul>
  <div class="menu-copyright"><p>Copyright© 2019-2020 | <a target="_blank" href="https://liushiya-github.github.io/">刘世亚</a> .AllRightsReserved</p></div>
</nav>
  <main id="main">
  <div class="container" id="container">
    <article class="article">
  <section class="head">
  <img   class="lazyload" data-original="/images/theme/post-image.jpg" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="  draggable="false">
  <div class="head-mask">
    <h1 class="head-title">ceph+docker方式搭建owncloud实现私网云盘</h1>
    <div class="head-info">
      <span class="post-info-item"><i class="iconfont iconcalendar"></i>2020-05-18</span
        class="post-info-item">
      
      <span class="post-info-item"><i class="iconfont iconeye"></i><span id="/2020/05/18/ceph+docker%E6%96%B9%E5%BC%8F%E6%90%AD%E5%BB%BAowncloud%E5%AE%9E%E7%8E%B0%E7%A7%81%E7%BD%91%E4%BA%91%E7%9B%98/"
          class="leancloud" data-flag-title="ceph+docker方式搭建owncloud实现私网云盘"></span></span>
      
      <span class="post-info-item"><i class="iconfont iconfont-size"></i>29416</span>
    </div>
  </div>
</section>
  <section class="main">
    <section class="content">
      <h3 id="ceph-docker方式搭建owncloud实现私网云盘"><a href="#ceph-docker方式搭建owncloud实现私网云盘" class="headerlink" title="ceph+docker方式搭建owncloud实现私网云盘"></a>ceph+docker方式搭建owncloud实现私网云盘</h3><h4 id="项目背景"><a href="#项目背景" class="headerlink" title="项目背景"></a>项目背景</h4><p>企业应用中越来越多的会使用容器技术来是实现解决方案</p>
<p>1、单节点的docker和docker-compose</p>
<p>2、多节点的docker swarm集群</p>
<p>3、多节点的k8s集群</p>
<p>以上都是可以基于容器技术实现企业应用的编排和快速部署</p>
<p>在特别大型的业务架构中, 数据达到T级别,P级别,甚至E级别或更高。如此大量 的数据,我们需要保证它的数据读写性能, 数据可靠性, 在线扩容等, 并且希望能 与容器技术整合使用。<br>ceph分布式存储是非常不错的开源解决方案<br>目前ceph在docker swarm集群与kubernetes集群中已经可以实现对接使用。 虽然3种类型存储并不是在任何场合都支持，但开源社区中也有相应的解决方 案，也会越来越成熟。<br>下面我们就使用ceph做底层存储, 分别在docker节点,docker swarm集 群,kubernetes集群中对接应用打造私有云盘与web应用。</p>
<h4 id="项目架构图"><a href="#项目架构图" class="headerlink" title="项目架构图"></a>项目架构图</h4><p><img class="lazyload" data-original="C:%5CUsers%5Cmy%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1587473152092.png" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="1587473152092"></p>
<p>PS:在架构图中写的比较全，但是在这个项目中不会全部做完，只是ceph集群加单台docker部署owncloud，实现云盘的功能，并没有docker集群的操作。</p>
<p> 四台主机</p>
<p>系统版本为7.6  1810</p>
<p>ceph1  10.0.0.5    作为管理ceph集群的节点，它自身也在集群中</p>
<p>ceph2  10.0.0.6      组成ceph集群</p>
<p>ceph3    10.0.0.41  组成ceph集群</p>
<p>owncloud  10.0.0.63   docker部署owncloud</p>
<h4 id="部署过程"><a href="#部署过程" class="headerlink" title="部署过程"></a>部署过程</h4><h5 id="准备过程"><a href="#准备过程" class="headerlink" title="准备过程"></a>准备过程</h5><h6 id="1、所有节点配置静态IP地址（要求能上公网）（ceph1举例）"><a href="#1、所有节点配置静态IP地址（要求能上公网）（ceph1举例）" class="headerlink" title="1、所有节点配置静态IP地址（要求能上公网）（ceph1举例）"></a>1、所有节点配置静态IP地址（要求能上公网）（ceph1举例）</h6><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 yum.repos.d 04:14:18]# cat /etc/sysconfig/network-scripts/ifcfg-eth0</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">PROXY_METHOD=none</span><br><span class="line">BROWSER_ONLY=no</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">DEFROUTE=yes</span><br><span class="line">IPV4_FAILURE_FATAL=no</span><br><span class="line">IPV6INIT=yes</span><br><span class="line">IPV6_AUTOCONF=yes</span><br><span class="line">IPV6_DEFROUTE=yes</span><br><span class="line">IPV6_FAILURE_FATAL=no</span><br><span class="line">IPV6_ADDR_GEN_MODE=stable-privacy</span><br><span class="line">NAME=eth0</span><br><span class="line">DEVICE=eth0</span><br><span class="line">ONBOOT=yes</span><br><span class="line">IPADDR=10.0.0.5</span><br><span class="line">NETMASK=255.255.255.0</span><br><span class="line">GATEWAY=10.0.0.254</span><br><span class="line">DNS1=223.5.5.5</span><br></pre></td></tr></tbody></table></figure>

<h6 id="2、所有节点主机名配置在-etc-hosts文件中"><a href="#2、所有节点主机名配置在-etc-hosts文件中" class="headerlink" title="2、所有节点主机名配置在/etc/hosts文件中"></a>2、所有节点主机名配置在/etc/hosts文件中</h6><p>（ceph1举例）</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 yum.repos.d 04:14:36]# cat /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">10.0.0.5 ceph1</span><br><span class="line">10.0.0.6 ceph2</span><br><span class="line">10.0.0.41 ceph3</span><br><span class="line">10.0.0.63 owncloud</span><br></pre></td></tr></tbody></table></figure>

<p>说明：还有没有开始部署的节点，也请先把IP地址和主机名进行绑定到/etc/hosts中</p>
<h6 id="3、所有节点关闭centos7的firewalld房防火墙，iptables规则清空"><a href="#3、所有节点关闭centos7的firewalld房防火墙，iptables规则清空" class="headerlink" title="3、所有节点关闭centos7的firewalld房防火墙，iptables规则清空"></a>3、所有节点关闭centos7的firewalld房防火墙，iptables规则清空</h6><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># systemctl stop firewalld # systemctl disable firewalld</span><br><span class="line"></span><br><span class="line"># yum install iptables-services -y # systemctl restart iptables # systemctl enable iptables</span><br><span class="line"></span><br><span class="line"># iptables -F # iptables -F -t nat # iptables -F -t mangle # iptables -F -t raw</span><br><span class="line"></span><br><span class="line"># service iptables save iptables: Saving firewall rules to /etc/sysconfig/iptables:[  OK  ]</span><br></pre></td></tr></tbody></table></figure>

<h6 id="4、所有节点关闭selinux"><a href="#4、所有节点关闭selinux" class="headerlink" title="4、所有节点关闭selinux"></a>4、所有节点关闭selinux</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 yum.repos.d 04:16:08]<span class="comment"># cat /etc/selinux/config</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This file controls the state of SELinux on the system.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># SELINUX= can take one of these three values:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># enforcing - SELinux security policy is enforced.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># permissive - SELinux prints warnings instead of enforcing.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># disabled - No SELinux policy is loaded.</span></span><br><span class="line"></span><br><span class="line">SELINUX=disabled</span><br><span class="line"></span><br><span class="line"><span class="comment"># SELINUXTYPE= can take one of three values:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># targeted - Targeted processes are protected,</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># minimum - Modification of targeted policy. Only selected processes are protected.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mls - Multi Level Security protection.</span></span><br><span class="line"></span><br><span class="line">SELINUXTYPE=targeted</span><br><span class="line"></span><br><span class="line">[root@ceph1 yum.repos.d 04:18:09]<span class="comment"># getenforce</span></span><br><span class="line">Disabled</span><br></pre></td></tr></tbody></table></figure>



<h6 id="5、同步时间"><a href="#5、同步时间" class="headerlink" title="5、同步时间"></a>5、同步时间</h6><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># systemctl restart ntpd </span><br><span class="line"># systemctl enabled ntpd</span><br></pre></td></tr></tbody></table></figure>

<p>PS：准备工作是在所有节点都进行操作的</p>
<h5 id="部署开始"><a href="#部署开始" class="headerlink" title="部署开始"></a>部署开始</h5><h6 id="1、添加ceph的yum源"><a href="#1、添加ceph的yum源" class="headerlink" title="1、添加ceph的yum源"></a>1、添加ceph的yum源</h6><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 yum.repos.d 02:40:54]# cat ceph.repo</span><br><span class="line">[ceph]</span><br><span class="line">name=ceph</span><br><span class="line">baseurl=https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-mimic/el7/x86_64/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[ceph-noarch]</span><br><span class="line">name=cephnoarch</span><br><span class="line">baseurl=https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-mimic/el7/noarch/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph source packages</span><br><span class="line">baseurl=https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-mimic/el7/SRPMS/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line">[root@ceph1 yum.repos.d 02:40:57]#</span><br></pre></td></tr></tbody></table></figure>

<p>注释：我这里使用的清华源，也可以使用阿里源或者是搭建本地yum源（搭建本地yum源的前提是将所有的rpm下载到本地）</p>
<p>将此ceph.repo拷贝到其他所有节点</p>
<h6 id="2、配置ceph集群ssh免密"><a href="#2、配置ceph集群ssh免密" class="headerlink" title="2、配置ceph集群ssh免密"></a>2、配置ceph集群ssh免密</h6><p>以ceph1为部署节点，在ceph1上配置到其他节点的ssh免密</p>
<p>目的：因为ceph集群会经常同步配置文件到其他节点，免密会方便很多</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ~]# ssh-keygen            # 三次回车做成空密码密钥[root@ceph_node1 ~]# ssh-copy-id ceph1 </span><br><span class="line">[root@ceph_node1 ~]# ssh-copy-id ceph2 </span><br><span class="line">[root@ceph_node1 ~]# ssh-copy-id ceph3 </span><br><span class="line">[root@ceph_node1 ~]# ssh-copy-id owncloud</span><br></pre></td></tr></tbody></table></figure>

<h6 id="3、在ceph1上安装部署工具"><a href="#3、在ceph1上安装部署工具" class="headerlink" title="3、在ceph1上安装部署工具"></a>3、在ceph1上安装部署工具</h6><p>其他节点不需要安装</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y  python-setuptools.noarch  <span class="comment">#先安装一个模块</span></span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install ceph-deploy -y  #安装部署工具</span><br></pre></td></tr></tbody></table></figure>

<h6 id="4、在ceph1上创建集群"><a href="#4、在ceph1上创建集群" class="headerlink" title="4、在ceph1上创建集群"></a>4、在ceph1上创建集群</h6><p>创建一个集群配置目录</p>
<p>注意：后面大部分的操作都在这个目录中操作</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /etc/ceph</span><br><span class="line"> <span class="built_in">cd</span> /etc/ceph</span><br></pre></td></tr></tbody></table></figure>

<p>创建一个ceph1集群</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy new  ceph1</span><br></pre></td></tr></tbody></table></figure>



<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 06:41:01]<span class="comment"># ll</span></span><br><span class="line">total 16</span><br><span class="line">-rw-r--r-- 1 root root  191 Apr 21 02:38 ceph.conf</span><br><span class="line">-rw-r--r-- 1 root root 2917 Apr 21 02:38 ceph-deploy-ceph.log</span><br><span class="line">-rw------- 1 root root   73 Apr 21 02:38 ceph.mon.keyring</span><br><span class="line">-rw-r--r-- 1 root root   92 Apr 16 12:47 rbdmap</span><br><span class="line">[root@ceph1 ceph 06:42:02]<span class="comment">#</span></span><br></pre></td></tr></tbody></table></figure>



<p>说明：</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ceph.conf 集群配置文件</span><br><span class="line"></span><br><span class="line">ceph-deploy-ceph.log  使用ceph-deploy部署的日志记录</span><br><span class="line"></span><br><span class="line">ceph.mon.keyring   验证key文件</span><br></pre></td></tr></tbody></table></figure>



<h6 id="5、安装ceph软件"><a href="#5、安装ceph软件" class="headerlink" title="5、安装ceph软件"></a>5、安装ceph软件</h6><p>在所有ceph集群节点安装ceph与ceph-radosgw软件包</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 06:42:32]# ceph -v</span><br><span class="line">ceph version 13.2.9 (58a2a9b31fd08d8bb3089fce0e312331502ff945) mimic (stable)</span><br><span class="line">[root@ceph1 ceph 06:42:36]#</span><br></pre></td></tr></tbody></table></figure>

<p>补充说明：如果网速够好的话，可以使用ceph-deploy install ceph1 ceph2 ceph3 命令来安装。ceph-deploy命令会自动通过公网官方源安装。（网速不给力就不要使用这种方式了）</p>
<p>在ceph客户端节点（k8s集群节点上）安装ceph-common</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yum install ceph-common -y</span></span><br></pre></td></tr></tbody></table></figure>



<h5 id="创建mon监控组件"><a href="#创建mon监控组件" class="headerlink" title="创建mon监控组件"></a>创建mon监控组件</h5><p>增加监控网络，网段为实验环境的集群物理网络</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 06:50:47]<span class="comment"># cat /etc/ceph/ceph.conf</span></span><br><span class="line">[global]</span><br><span class="line">fsid = bb1d01eb-0d72-4794-9d7c-cb692d7a8f34</span><br><span class="line">mon_initial_members = ceph1</span><br><span class="line">mon_host = 10.0.0.5</span><br><span class="line">auth_cluster_required = cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br><span class="line"></span><br><span class="line">public network = 10.0.0.0/24  <span class="comment">#添加的配置，监控网络</span></span><br></pre></td></tr></tbody></table></figure>

<h6 id="1、集群节点初始化"><a href="#1、集群节点初始化" class="headerlink" title="1、集群节点初始化"></a>1、集群节点初始化</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 06:52:13]<span class="comment"># ceph-deploy mon create-initial</span></span><br><span class="line">[root@ceph1 ceph 06:52:05]<span class="comment"># ceph health</span></span><br><span class="line">HEALTH_OK</span><br></pre></td></tr></tbody></table></figure>

<h6 id="2、将配置文件信息同步到所有的ceph集群节点"><a href="#2、将配置文件信息同步到所有的ceph集群节点" class="headerlink" title="2、将配置文件信息同步到所有的ceph集群节点"></a>2、将配置文件信息同步到所有的ceph集群节点</h6><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy admin ceph1 ceph2 ceph3</span><br></pre></td></tr></tbody></table></figure>



<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[root@ceph1 ceph 06:57:15]<span class="comment"># ceph -s</span></span><br><span class="line">  cluster:</span><br><span class="line">    id:     bb1d01eb-0d72-4794-9d7c-cb692d7a8f34</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum ceph1</span><br><span class="line">    mgr: no daemons active</span><br><span class="line">    osd: 0 osds: 0 up, 0 <span class="keyword">in</span></span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0  objects, 0 B</span><br><span class="line">    usage:   0 B used, 0 B / 0 B avail</span><br><span class="line">    pgs:</span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph 06:57:19]<span class="comment">#</span></span><br></pre></td></tr></tbody></table></figure>

<p>此时集群已经搭建好了</p>
<h6 id="3、多个mon节点"><a href="#3、多个mon节点" class="headerlink" title="3、多个mon节点"></a>3、多个mon节点</h6><p>为了防止mon节点单点故障，你可以添加多个mon节点（非必要步骤）</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy mon add ceph2</span><br><span class="line">ceph-deploy mon add ceph3</span><br></pre></td></tr></tbody></table></figure>



<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 06:59:11]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     bb1d01eb-0d72-4794-9d7c-cb692d7a8f34</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line">健康状态为ok</span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph1,ceph2,ceph3  三个监控</span><br><span class="line">    mgr: no daemons active</span><br><span class="line">    osd: 0 osds: 0 up, 0 in</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0  objects, 0 B</span><br><span class="line">    usage:   0 B used, 0 B / 0 B avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></tbody></table></figure>





<h5 id="监控到时间不同不的解决方法"><a href="#监控到时间不同不的解决方法" class="headerlink" title="监控到时间不同不的解决方法"></a>监控到时间不同不的解决方法</h5><p>ceph集群对时间同步的要求非常高，即使你已经将ntpd服务开启，但仍然有可能会有clock skew deteted相关警告</p>
<p>可以尝试如下做法：</p>
<h6 id="1、在ceph集群节点上使用crontab同步"><a href="#1、在ceph集群节点上使用crontab同步" class="headerlink" title="1、在ceph集群节点上使用crontab同步"></a>1、在ceph集群节点上使用crontab同步</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># systemctl stop ntpd # systemctl disable ntpd</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># crontab -e */10 * * * * ntpdate ntp1.aliyun.com                每5或10 分钟同步1次公网的任意时间服务器</span></span><br></pre></td></tr></tbody></table></figure>

<h6 id="2、调大时间警告的阈值"><a href="#2、调大时间警告的阈值" class="headerlink" title="2、调大时间警告的阈值"></a>2、调大时间警告的阈值</h6><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph_node1 ceph]# vim ceph.conf </span><br><span class="line">[global]                                在global参数组里添加 以下两行                       ...... </span><br><span class="line">mon clock drift allowed = 2             # monitor间的时钟滴 答数(默认0.5秒) </span><br><span class="line">mon clock drift warn backoff = 30       # 调大时钟允许的偏移量 (默认为5)</span><br></pre></td></tr></tbody></table></figure>

<h6 id="3、同步到所有节点"><a href="#3、同步到所有节点" class="headerlink" title="3、同步到所有节点"></a>3、同步到所有节点</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph_node1 ceph]<span class="comment"># ceph-deploy --overwrite-conf admin ceph_node1 ceph_node2 ceph_node3</span></span><br><span class="line"></span><br><span class="line">前面第1次同步不需要加--overwrite-conf参数 这次修改ceph.conf再同步就需要加--overwrite-conf参数覆盖</span><br></pre></td></tr></tbody></table></figure>

<h6 id="4、所有ceph集群节点上重启ceph-mon-target服务"><a href="#4、所有ceph集群节点上重启ceph-mon-target服务" class="headerlink" title="4、所有ceph集群节点上重启ceph-mon.target服务"></a>4、所有ceph集群节点上重启ceph-mon.target服务</h6><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl  restart  ceph-mon.target</span><br></pre></td></tr></tbody></table></figure>

<h5 id="创建mgr管理组件"><a href="#创建mgr管理组件" class="headerlink" title="创建mgr管理组件"></a>创建mgr管理组件</h5><p>ceph luminous版本之后新增加了一个组件：ceph  manager daemon ，简称ceph-mgr</p>
<p>该组件的主要作用是分担和扩展monitor的部分功能，减轻monitor的负担，让更好的管理ceph存储系统</p>
<p>ceph dashboard图形化管理就用到了mgr</p>
<h6 id="1、创建一个mgr"><a href="#1、创建一个mgr" class="headerlink" title="1、创建一个mgr"></a>1、创建一个mgr</h6><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 07:11:17]# ceph-deploy mgr  create ceph1</span><br></pre></td></tr></tbody></table></figure>

<p>查看一下</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[root@ceph1 ceph 07:11:32]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     bb1d01eb-0d72-4794-9d7c-cb692d7a8f34</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph1,ceph2,ceph3</span><br><span class="line">    mgr: ceph1(active) 这里就是mgr </span><br><span class="line">    osd: 0 osds: 0 up, 0 in</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0  objects, 0 B</span><br><span class="line">    usage:   0 B used, 0 B / 0 B avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></tbody></table></figure>

<h6 id="2、添加多个mgr可以实现HA"><a href="#2、添加多个mgr可以实现HA" class="headerlink" title="2、添加多个mgr可以实现HA"></a>2、添加多个mgr可以实现HA</h6><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy mgr  create ceph2</span><br><span class="line">ceph-deploy mgr  create ceph3</span><br></pre></td></tr></tbody></table></figure>



<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 07:13:12]<span class="comment"># ceph -s</span></span><br><span class="line">  cluster:</span><br><span class="line">    id:     bb1d01eb-0d72-4794-9d7c-cb692d7a8f34</span><br><span class="line">    health: HEALTH_WARN</span><br><span class="line">            OSD count 0 &lt; osd_pool_default_size 3</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph1,ceph2,ceph3  三个监控</span><br><span class="line">    mgr: ceph1(active), standbys: ceph2, ceph3    ceph1为主mgr</span><br><span class="line">    osd: 0 osds: 0 up, 0 <span class="keyword">in</span></span><br><span class="line">看到0个磁盘</span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0  objects, 0 B</span><br><span class="line">    usage:   0 B used, 0 B / 0 B avail</span><br><span class="line">    pgs:</span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph 07:13:19]<span class="comment">#</span></span><br></pre></td></tr></tbody></table></figure>

<p>至此ceph集群基本已经搭建完成，但是还需要增加OSD磁盘</p>
<h5 id="ceph集群管理"><a href="#ceph集群管理" class="headerlink" title="ceph集群管理"></a>ceph集群管理</h5><h6 id="1、物理上添加磁盘"><a href="#1、物理上添加磁盘" class="headerlink" title="1、物理上添加磁盘"></a>1、物理上添加磁盘</h6><p>在ceph集群所有节点上增加磁盘</p>
<p>（我这里只是模拟，就增加一个5G的做测试使用)</p>
<p><img class="lazyload" data-original="C:%5CUsers%5Cmy%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1587468133938.png" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="1587468133938"></p>
<p>ceph1 ceph2 ceph3 三个节点上都要进行操作</p>
<p>不重启让系统识别新增加的磁盘</p>
<p>操作之前</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 07:13:19]<span class="comment"># lsblk</span></span><br><span class="line">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda               8:0    0   50G  0 disk</span><br><span class="line">├─sda1            8:1    0    1G  0 part /boot</span><br><span class="line">└─sda2            8:2    0   49G  0 part</span><br><span class="line">  ├─centos-swap 253:0    0    2G  0 lvm  [SWAP]</span><br><span class="line">  └─centos-root 253:1    0   47G  0 lvm  /</span><br><span class="line">sr0              11:0    1  4.3G  0 rom</span><br><span class="line">[root@ceph1 ceph 07:24:28]<span class="comment">#</span></span><br></pre></td></tr></tbody></table></figure>

<p>操作之中</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 07:24:28]# echo "- - -" &gt; /sys/class/scsi_host/host0/scan</span><br><span class="line">[root@ceph1 ceph 07:26:01]# echo "- - -" &gt; /sys/class/scsi_host/host1/scan</span><br><span class="line">[root@ceph1 ceph 07:26:08]# echo "- - -" &gt; /sys/class/scsi_host/host2/scan</span><br></pre></td></tr></tbody></table></figure>

<p>操作之后</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 07:26:12]<span class="comment"># lsblk</span></span><br><span class="line">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda               8:0    0   50G  0 disk</span><br><span class="line">├─sda1            8:1    0    1G  0 part /boot</span><br><span class="line">└─sda2            8:2    0   49G  0 part</span><br><span class="line">  ├─centos-swap 253:0    0    2G  0 lvm  [SWAP]</span><br><span class="line">  └─centos-root 253:1    0   47G  0 lvm  /</span><br><span class="line">sdb       这里就是我们新加的        8:16   0    5G  0 disk</span><br><span class="line">sr0              11:0    1  4.3G  0 rom</span><br><span class="line">[root@ceph1 ceph 07:26:36]<span class="comment">#</span></span><br></pre></td></tr></tbody></table></figure>



<h6 id="2、创建OSD磁盘"><a href="#2、创建OSD磁盘" class="headerlink" title="2、创建OSD磁盘"></a>2、创建OSD磁盘</h6><p>将磁盘添加到ceph集群中需要osd</p>
<p>ceph OSD ：功能是存储于处理一些数据，并通过检查其他OSD守护进程的心跳来向ceph monitors 提供一些监控信息</p>
<p>1、列表所有节点的磁盘 并使用zap命令清除磁盘信息准备创建OSD</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> ceph-deploy disk zap ceph1 /dev/sdb</span><br><span class="line">ceph-deploy disk zap ceph2 /dev/sdb</span><br><span class="line">ceph-deploy disk zap ceph3 /dev/sdb</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 07:32:01]# ceph-deploy disk zap ceph3 /dev/sdb</span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO ] Invoked (2.0.1): /usr/bin/ceph-deploy disk zap ceph3 /dev/sdb</span><br><span class="line">[ceph_deploy.cli][INFO ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  debug                         : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  subcommand                    : zap</span><br><span class="line">[ceph_deploy.cli][INFO ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7feb12eb0368&gt;</span><br><span class="line">[ceph_deploy.cli][INFO ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO ]  host                          : ceph3</span><br><span class="line">[ceph_deploy.cli][INFO ]  func                          : &lt;function disk at 0x7feb12eec8c0&gt;</span><br><span class="line">[ceph_deploy.cli][INFO ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  default_release               : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  disk                          : ['/dev/sdb']</span><br><span class="line">[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph3</span><br><span class="line">[ceph3][DEBUG ] connected to host: ceph3</span><br><span class="line">[ceph3][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph3][DEBUG ] detect machine type</span><br><span class="line">[ceph3][DEBUG ] find the location of an executable</span><br><span class="line">[ceph_deploy.osd][INFO ] Distro info: CentOS Linux 7.6.1810 Core</span><br><span class="line">[ceph3][DEBUG ] zeroing last few blocks of device</span><br><span class="line">[ceph3][DEBUG ] find the location of an executable</span><br><span class="line">[ceph3][INFO ] Running command: /usr/sbin/ceph-volume lvm zap /dev/sdb</span><br><span class="line">[ceph3][WARNIN] --&gt; Zapping: /dev/sdb</span><br><span class="line">[ceph3][WARNIN] --&gt; --destroy was not specified, but zapping a whole device will remove the partition table</span><br><span class="line">[ceph3][WARNIN] Running command: /bin/dd if=/dev/zero of=/dev/sdb bs=1M count=10 conv=fsync</span><br><span class="line">[ceph3][WARNIN] --&gt; Zapping successful for: &lt;Raw Device: /dev/sdb&gt;</span><br></pre></td></tr></tbody></table></figure>

<p>2、创建OSD磁盘</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy osd create --data /dev/sdb ceph1</span><br><span class="line">ceph-deploy osd create --data /dev/sdb ceph2</span><br><span class="line">ceph-deploy osd create --data /dev/sdb ceph3</span><br></pre></td></tr></tbody></table></figure>

<p>3、验证</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 07:34:31]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     bb1d01eb-0d72-4794-9d7c-cb692d7a8f34</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph1,ceph2,ceph3</span><br><span class="line">    mgr: ceph1(active), standbys: ceph2, ceph3</span><br><span class="line">    osd: 3 osds: 3 up, 3 in</span><br><span class="line">三个磁盘</span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0  objects, 0 B</span><br><span class="line">    usage:   3.0 GiB used, 12 GiB / 15 GiB avail  合成一个15G的磁盘，但是使用了3G 是因为合成时会用掉一些空间</span><br><span class="line">    pgs:</span><br></pre></td></tr></tbody></table></figure>

<h5 id="扩容操作"><a href="#扩容操作" class="headerlink" title="扩容操作"></a>扩容操作</h5><h6 id="节点中增加某个磁盘"><a href="#节点中增加某个磁盘" class="headerlink" title="节点中增加某个磁盘"></a>节点中增加某个磁盘</h6><p>如：在ceph3上再添加一块磁盘/dev/sdc。做如下操作就可以</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy disk zap ceph3 /dev/sdc</span><br><span class="line">ceph-deploy osd create --data /dev/sdc ceph3</span><br></pre></td></tr></tbody></table></figure>

<p>验证</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 08:10:41]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     bb1d01eb-0d72-4794-9d7c-cb692d7a8f34</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph1,ceph2,ceph3</span><br><span class="line">    mgr: ceph1(active), standbys: ceph2, ceph3</span><br><span class="line">    osd: 4 osds: 4 up, 4 in</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0  objects, 0 B</span><br><span class="line">    usage:   4.0 GiB used, 16 GiB / 20 GiB avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></tbody></table></figure>

<h6 id="增加某个节点并添加此节点的磁盘"><a href="#增加某个节点并添加此节点的磁盘" class="headerlink" title="增加某个节点并添加此节点的磁盘"></a>增加某个节点并添加此节点的磁盘</h6><p>如果是新增加一个节点（假设是owncloud节点）并添加一块磁盘</p>
<p>那么做法如下：</p>
<p>1、准备好owncloud节点的基本环境，安装ceph相关软件</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y  ceph  ceph-radosgw -y</span><br></pre></td></tr></tbody></table></figure>

<p>2、在ceph1上同步1配置到owncloud节点</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy admin  owncloud</span><br></pre></td></tr></tbody></table></figure>

<p>3、将owncloud加入集群</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy disk  zap  owncloud /dev/sdb</span><br><span class="line">ceph-deploy osd create --data   /dev/sdb  owncloud</span><br></pre></td></tr></tbody></table></figure>

<p>4、验证</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 08:15:44]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     bb1d01eb-0d72-4794-9d7c-cb692d7a8f34</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph1,ceph2,ceph3</span><br><span class="line">    mgr: ceph1(active), standbys: ceph2, ceph3</span><br><span class="line">    osd: 5 osds: 5 up, 5 in</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0  objects, 0 B</span><br><span class="line">    usage:   5.0 GiB used, 20 GiB / 25 GiB avail</span><br><span class="line">    pgs:</span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph 08:15:48]#</span><br></pre></td></tr></tbody></table></figure>



<h6 id="删除磁盘方法"><a href="#删除磁盘方法" class="headerlink" title="删除磁盘方法"></a>删除磁盘方法</h6><p>和很多存储一样，增加磁盘（扩容）都比较方便，但是要删除磁盘会比较麻烦。</p>
<p>这里以删除owncloud节点的/dev/sdb磁盘为例</p>
<p>1、查看osd磁盘状态</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 08:23:49]# ceph  osd  tree</span><br><span class="line">ID CLASS WEIGHT  TYPE NAME         STATUS REWEIGHT PRI-AFF</span><br><span class="line">-1       0.02449 root default</span><br><span class="line">-3       0.00490     host ceph1</span><br><span class="line"> 0   hdd 0.00490         osd.0         up  1.00000 1.00000</span><br><span class="line">-5       0.00490     host ceph2</span><br><span class="line"> 1   hdd 0.00490         osd.1         up  1.00000 1.00000</span><br><span class="line">-7       0.00980     host ceph3</span><br><span class="line"> 2   hdd 0.00490         osd.2         up  1.00000 1.00000</span><br><span class="line"> 3   hdd 0.00490         osd.3         up  1.00000 1.00000</span><br><span class="line">-9       0.00490     host owncloud</span><br><span class="line"> 4   hdd 0.00490         osd.4         up  1.00000 1.00000</span><br></pre></td></tr></tbody></table></figure>

<p>2、先标记为out</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 08:23:56]# ceph osd out osd.4</span><br><span class="line">marked out osd.4.</span><br><span class="line">[root@ceph1 ceph 08:24:46]# ceph  osd  tree</span><br><span class="line">ID CLASS WEIGHT  TYPE NAME         STATUS REWEIGHT PRI-AFF</span><br><span class="line">-1       0.02449 root default</span><br><span class="line">-3       0.00490     host ceph1</span><br><span class="line"> 0   hdd 0.00490         osd.0         up  1.00000 1.00000</span><br><span class="line">-5       0.00490     host ceph2</span><br><span class="line"> 1   hdd 0.00490         osd.1         up  1.00000 1.00000</span><br><span class="line">-7       0.00980     host ceph3</span><br><span class="line"> 2   hdd 0.00490         osd.2         up  1.00000 1.00000</span><br><span class="line"> 3   hdd 0.00490         osd.3         up  1.00000 1.00000</span><br><span class="line">-9       0.00490     host owncloud</span><br><span class="line"> 4   hdd 0.00490         osd.4         up        0 1.00000</span><br><span class="line"> 可以看到权重为0但是状态还是up</span><br></pre></td></tr></tbody></table></figure>

<p>3、再rm删除，但是要先去osd.4对应的节点上停止ceph-osd服务，否则删除不了</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@owncloud yum.repos.d 08:27:35]# systemctl stop  ceph-osd@4.service</span><br></pre></td></tr></tbody></table></figure>



<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 08:28:09]# ceph osd rm osd.4</span><br><span class="line"></span><br><span class="line">removed osd.4</span><br></pre></td></tr></tbody></table></figure>

<p>4、查看集群状态</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 08:28:18]#</span><br><span class="line">[root@ceph1 ceph 08:28:18]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     bb1d01eb-0d72-4794-9d7c-cb692d7a8f34</span><br><span class="line">    health: HEALTH_WARN 会有警告，是因为没有在crush算法中删除</span><br><span class="line">            1 osds exist in the crush map but not in the osdmap</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph1,ceph2,ceph3 </span><br><span class="line">    mgr: ceph1(active), standbys: ceph2, ceph3</span><br><span class="line">    osd: 4 osds: 4 up, 4 in</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0  objects, 0 B</span><br><span class="line">    usage:   4.0 GiB used, 16 GiB / 20 GiB avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></tbody></table></figure>

<p>此时再查看，发现osd.4已经没有up了</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 08:28:22]# ceph osd  tree</span><br><span class="line">ID CLASS WEIGHT  TYPE NAME         STATUS REWEIGHT PRI-AFF</span><br><span class="line">-1       0.02449 root default</span><br><span class="line">-3       0.00490     host ceph1</span><br><span class="line"> 0   hdd 0.00490         osd.0         up  1.00000 1.00000</span><br><span class="line">-5       0.00490     host ceph2</span><br><span class="line"> 1   hdd 0.00490         osd.1         up  1.00000 1.00000</span><br><span class="line">-7       0.00980     host ceph3</span><br><span class="line"> 2   hdd 0.00490         osd.2         up  1.00000 1.00000</span><br><span class="line"> 3   hdd 0.00490         osd.3         up  1.00000 1.00000</span><br><span class="line">-9       0.00490     host owncloud</span><br><span class="line"> 4   hdd 0.00490         osd.4        DNE        0</span><br><span class="line">[root@ceph1 ceph 08:29:23]#</span><br></pre></td></tr></tbody></table></figure>



<p>5、在crush算法中和auth验证中删除</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@owncloud yum.repos.d 08:32:29]# ceph osd crush remove osd.4</span><br><span class="line">removed item id 4 name 'osd.4' from crush map</span><br><span class="line">[root@owncloud yum.repos.d 08:32:43]# ceph auth del osd.4</span><br><span class="line">updated</span><br></pre></td></tr></tbody></table></figure>

<p>6、还需要在osd.4对应的节点上卸载磁盘</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@owncloud yum.repos.d 08:33:04]# df -Th |grep  osd</span><br><span class="line">tmpfs                   tmpfs     487M   52K  487M   1% /var/lib/ceph/osd/ceph-4</span><br><span class="line"></span><br><span class="line">[root@owncloud yum.repos.d 08:33:30]# umount /var/lib/ceph/osd/ceph-4/</span><br><span class="line">[root@owncloud yum.repos.d 08:33:50]# df -Th</span><br><span class="line">Filesystem              Type      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/centos-root xfs        47G  1.6G   46G   4% /</span><br><span class="line">devtmpfs                devtmpfs  475M     0  475M   0% /dev</span><br><span class="line">tmpfs                   tmpfs     487M     0  487M   0% /dev/shm</span><br><span class="line">tmpfs                   tmpfs     487M   14M  473M   3% /run</span><br><span class="line">tmpfs                   tmpfs     487M     0  487M   0% /sys/fs/cgroup</span><br><span class="line">/dev/sda1               xfs      1014M  133M  882M  14% /boot</span><br><span class="line">tmpfs                   tmpfs      98M     0   98M   0% /run/user/0</span><br></pre></td></tr></tbody></table></figure>



<p>在ceph1节点查看验证</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 08:29:23]# ceph osd  tree</span><br><span class="line">ID CLASS WEIGHT  TYPE NAME         STATUS REWEIGHT PRI-AFF</span><br><span class="line">-1       0.01959 root default</span><br><span class="line">-3       0.00490     host ceph1</span><br><span class="line"> 0   hdd 0.00490         osd.0         up  1.00000 1.00000</span><br><span class="line">-5       0.00490     host ceph2</span><br><span class="line"> 1   hdd 0.00490         osd.1         up  1.00000 1.00000</span><br><span class="line">-7       0.00980     host ceph3</span><br><span class="line"> 2   hdd 0.00490         osd.2         up  1.00000 1.00000</span><br><span class="line"> 3   hdd 0.00490         osd.3         up  1.00000 1.00000</span><br><span class="line">-9             0     host owncloud</span><br></pre></td></tr></tbody></table></figure>



<p>到这里，就完全删除了。</p>
<p>如果想要加回来，再次部署在节点上即可</p>
<p>如下命令</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# ceph-deploy disk zap ceph1 /dev/sdb </span><br><span class="line">[root@ceph1 ceph]# ceph-deploy osd create --data /dev/sdb ceph1</span><br></pre></td></tr></tbody></table></figure>



<h5 id="小结与说明"><a href="#小结与说明" class="headerlink" title="小结与说明"></a>小结与说明</h5><p>ceph集群安装要注意：</p>
<p>1、ceph-deploy命令默认找官方yum源，国内网速非常慢（而且ceph安装时设置了300秒超时，也就是你安装5分钟内不安装完就会断开）</p>
<p>2、建议使用国内镜像源（我使用的清华源），如果网速还是慢，就做好本地yum源。</p>
<p>3、ceph集群节点需要安装ceph与ceph-radosgw软件包，客户端节点只需要安装ceph-common软件包</p>
<p>4、ceph集群对时间同步很敏感，请一定保证没有时间不同的相关警告</p>
<p>ceph集群操作注意：</p>
<p>任何操作一定要保证集群健康的情况下操作，并使用ceph -s命令进行确认</p>
<p>修改配置文件请在部署节点上修改，然后使用</p>
<p>ceph-deploy –overwriteconf admin ceph1 ceph2 ceph3命令同步到其它节点</p>
<h5 id="ceph的存储类型"><a href="#ceph的存储类型" class="headerlink" title="ceph的存储类型"></a>ceph的存储类型</h5><h6 id="三种存储类型介绍"><a href="#三种存储类型介绍" class="headerlink" title="三种存储类型介绍"></a>三种存储类型介绍</h6><p><img class="lazyload" data-original="C:%5CUsers%5Cmy%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1587473493263.png" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="1587473493263"></p>
<p>无论用那种存储，都可以分为以下三种类型</p>
<p>文件存储：类似于一个大的目录，多个客户端可以挂载过来</p>
<p>优点：利于数据共享</p>
<p>缺点：速度较慢</p>
<p>块存储：类似于一个block的设备，客户端可以格式化，挂载并使用，和用一个硬盘一样</p>
<p>优点：和本地硬盘一样直接使用</p>
<p>缺点：数据不同享</p>
<p>对象存储：看做一个综合了文件存储和块存储优点的大硬盘，可以挂载也可以通过URL来访问</p>
<p>优点：速度快，数据共享</p>
<p>缺点：成本高，不兼容现有的模式</p>
<p><img class="lazyload" data-original="C:%5CUsers%5Cmy%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1587473700624.png" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="1587473700624"></p>
<p>RADOS;ceph的高可靠，高可扩展、高性能、高自动化都是由这一层来提供的，用户数据的存储最终也是通过这一层来进行存储的</p>
<p>可以说RADOS就是ceph底层的原生态的数据引擎，但是实际应用时却不直接使用它，而是分为以下4种方式来使用：</p>
<p>LIBRADOS是一个库, 它允许应用程序通过访问该库来与RADOS系统进行交 互，支持多种编程语言。如Python,C,C++等. 简单来说,就是给开发人员使 用的接口。</p>
<p>CEPH FS通过Linux内核客户端和FUSE来提供文件系统。(文件存储)</p>
<p>RBD通过Linux内核客户端和QEMU/KVM驱动来提供一个分布式的块设 备。(块存储) </p>
<p>RADOSGW是一套基于当前流行的RESTFUL协议的网关，并且兼容S3和 Swift。(对象存储)</p>
<h6 id="存储原理"><a href="#存储原理" class="headerlink" title="存储原理"></a>存储原理</h6><p>要实现数据存取需要建一个pool,创建pool要分配PG。</p>
<p><img class="lazyload" data-original="C:%5CUsers%5Cmy%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1587473929074.png" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="1587473929074"></p>
<p>如果客户端对一个pool写了一个文件，那么这个文件是如何分布到多个节点的磁盘上呢？</p>
<p>答案是通过crush算法</p>
<h6 id="CRUSH算法"><a href="#CRUSH算法" class="headerlink" title="CRUSH算法"></a>CRUSH算法</h6><p>CRUSH(Controlled Scalable Decentralized Placement of Replicated Data)算法为可控的,可扩展的,分布式的副本数据放置算法的简称。 PG到OSD的映射的过程算法叫做CRUSH 算法。(一个Object需要保存三个 副本，也就是需要保存在三个osd上)。</p>
<p>CRUSH算法是一个伪随机的过程，他可以从所有的OSD中，随机性选择一 个OSD集合，但是同一个PG每次随机选择的结果是不变的，也就是映射的 OSD集合是固定的。</p>
<h5 id="小结："><a href="#小结：" class="headerlink" title="小结："></a>小结：</h5><p>1、客户端直接对pool操作(但文件存储、块存储、对象存储区我们不这么做)</p>
<p>2、pool里面要分配PG</p>
<p>3、PG里面可以存放多个对象</p>
<p>4、对象就是客户端写入的数据分离单位</p>
<p>5、crush算法将客户端写入的数据映射分布到OSD，从而最终存放到物理磁盘上。</p>
<h5 id="RADOS原生数据存取"><a href="#RADOS原生数据存取" class="headerlink" title="RADOS原生数据存取"></a>RADOS原生数据存取</h5><h6 id="1、创建pool并测试"><a href="#1、创建pool并测试" class="headerlink" title="1、创建pool并测试"></a>1、创建pool并测试</h6><p>创建test_pool 指定pg数为128</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 09:03:17]# ceph osd pool create test_pool 128</span><br><span class="line">pool 'test_pool' created</span><br></pre></td></tr></tbody></table></figure>

<p>修改pg数量，可以使用修改调整</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set test_pool  pg_num 64</span><br></pre></td></tr></tbody></table></figure>

<p>查看数量</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 09:03:32]# ceph osd pool  get test_pool pg_num</span><br><span class="line">pg_num: 128</span><br><span class="line">[root@ceph1 ceph 09:03:46]#</span><br></pre></td></tr></tbody></table></figure>



<p>说明：pg数与osd数量关系</p>
<p>pg数为2的倍数，一般为5个一下osd,分为128个PG或者一下即可</p>
<p>可以使用ceph  osd  pool set test_pool pg_num 64 这样的命令来尝试调整。</p>
<h6 id="2、存储测试"><a href="#2、存储测试" class="headerlink" title="2、存储测试"></a>2、存储测试</h6><p>1、我这里把本机的/etc/fstab文件上传到test_pool并取名为netfatab</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 09:11:12]# rados put newfatab /etc/fstab  --pool=test_pool</span><br></pre></td></tr></tbody></table></figure>

<p>2、查看</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 09:11:19]# rados -p test_pool ls</span><br><span class="line">newfatab</span><br><span class="line">[root@ceph1 ceph 09:11:47]#</span><br></pre></td></tr></tbody></table></figure>



<p>3、删除并查看</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 09:12:45]# rados rm newfatab --pool=test_pool</span><br><span class="line">[root@ceph1 ceph 09:13:16]# rados -p test_pool ls</span><br></pre></td></tr></tbody></table></figure>

<h6 id="3、删除pool"><a href="#3、删除pool" class="headerlink" title="3、删除pool"></a>3、删除pool</h6><p>1、在部署节点ceph1上增加参数允许ceph删除pool</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 09:15:29]# cat ceph.conf</span><br><span class="line">[global]</span><br><span class="line">fsid = bb1d01eb-0d72-4794-9d7c-cb692d7a8f34</span><br><span class="line">mon_initial_members = ceph1</span><br><span class="line">mon_host = 10.0.0.5</span><br><span class="line">auth_cluster_required = cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br><span class="line"></span><br><span class="line">public network = 10.0.0.0/24</span><br><span class="line">mon_allow_pool_delete = true</span><br><span class="line">增加最后一条信息</span><br></pre></td></tr></tbody></table></figure>

<p>2、修改了配置，要同步到其他集群节点</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 09:17:19]# ceph-deploy --overwrite-conf admin ceph1 ceph2 ceph3</span><br></pre></td></tr></tbody></table></figure>

<p>3、重启监控服务</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 09:17:42]# systemctl restart  ceph-mon.target</span><br></pre></td></tr></tbody></table></figure>

<p>4、删除pool名输入两次，后接–yes-i-really-really-mean-it参数就可以删 除了</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 09:17:52]# ceph osd pool delete test_pool test_pool --yes-i-really-really-mean-it</span><br><span class="line">pool 'test_pool' removed</span><br><span class="line">[root@ceph1 ceph 09:19:23]#</span><br></pre></td></tr></tbody></table></figure>

<h5 id="创建ceph文件存储"><a href="#创建ceph文件存储" class="headerlink" title="创建ceph文件存储"></a>创建ceph文件存储</h5><p>要运行ceph文件系统，你必须先创建至少一个带mds的ceph存储集群</p>
<p>（ceph块设备和ceph对象存储都不使用MDS）</p>
<p>ceph MDS为ceph文件存储类型存放元数据metadata</p>
<p>1、在ceph1节点部署上同步配置文件，并创建MDS（也可以做多个mds实现HA）</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy mds  create ceph1 ceph2 ceph3</span><br></pre></td></tr></tbody></table></figure>

<p>2、一个ceph文件系统需要至少两个RADOS存储池，一个用于数据，一个用于元数据。所以我们创建他们</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 09:24:47]# ceph osd  pool create cephfs_pool 128</span><br><span class="line">pool 'cephfs_pool' created</span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph 09:25:06]# ceph osd pool create cephfs_metadata 64</span><br><span class="line">pool 'cephfs_metadata' created</span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph 09:25:27]# ceph osd pool ls |grep  cephfs</span><br><span class="line">cephfs_pool</span><br><span class="line">cephfs_metadata</span><br></pre></td></tr></tbody></table></figure>



<p>3、创建ceph文件系统并确认客户端访问的节点</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 09:26:38]# ceph fs new cephfs cephfs_metadata cephfs_pool</span><br><span class="line">new fs with metadata pool 3 and data pool 2</span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph 09:26:59]# ceph fs ls</span><br><span class="line">name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_pool ]</span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph 09:27:09]# ceph mds stat</span><br><span class="line">cephfs-1/1/1 up  {0=ceph3=up:active}, 2 up:standby</span><br><span class="line">[root@ceph1 ceph 09:27:17]#</span><br></pre></td></tr></tbody></table></figure>

<p>4、客户端准备key验证文件</p>
<p>说明：ceph默认启用了cephx认证，所以客户端的挂载必须要认证</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@lsy ~ 09:52:52]# mount -t ceph  10.0.0.5:6789:/ /mnt -o name=admin,secret=AQBQ0J5eLDHnLRAAf850/HEvbZd3DAvWE8czrA==</span><br><span class="line">[root@lsy ~ 09:54:04]# df -Th</span><br><span class="line">Filesystem              Type      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/centos-root xfs        47G  3.6G   44G   8% /</span><br><span class="line">devtmpfs                devtmpfs  475M     0  475M   0% /dev</span><br><span class="line">tmpfs                   tmpfs     487M     0  487M   0% /dev/shm</span><br><span class="line">tmpfs                   tmpfs     487M   14M  473M   3% /run</span><br><span class="line">tmpfs                   tmpfs     487M     0  487M   0% /sys/fs/cgroup</span><br><span class="line">/dev/sda1               xfs      1014M  133M  882M  14% /boot</span><br><span class="line">tmpfs                   tmpfs      98M     0   98M   0% /run/user/0</span><br><span class="line">10.0.0.5:6789:/         ceph      5.0G     0  5.0G   0% /mnt</span><br><span class="line">[root@lsy ~ 09:54:13]#</span><br></pre></td></tr></tbody></table></figure>

<p>这样的方式可以</p>
<p>但是使用文件密钥的方式我就实现不出来。</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@lsy ~ 10:05:42]# mount -t ceph ceph1:6789:/   /mnt -o name=admin,secretfile=/root/admin.key</span><br><span class="line">mount: wrong fs type, bad option, bad superblock on ceph1:6789:/,</span><br><span class="line">       missing codepage or helper program, or other error</span><br><span class="line"></span><br><span class="line">   In some cases useful info is found in syslog - try</span><br><span class="line">   dmesg | tail or so.</span><br></pre></td></tr></tbody></table></figure>

<p>几种方式的总结：见博客</p>
<p><a href="https://blog.csdn.net/weixin_42506599/article/details/105669234" target="_blank" rel="noopener">https://blog.csdn.net/weixin_42506599/article/details/105669234</a></p>
<h5 id="删除文件存储方法"><a href="#删除文件存储方法" class="headerlink" title="删除文件存储方法"></a>删除文件存储方法</h5><p>1、在客户端上删除数据，并umount所有挂载</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@lsy ~ 10:24:55]# rm /mnt/* -rf</span><br><span class="line">[root@lsy ~ 10:25:04]#</span><br><span class="line">[root@lsy ~ 10:25:05]# umount /mnt/</span><br><span class="line">[root@lsy ~ 10:25:09]# df -Th</span><br><span class="line">Filesystem              Type      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/centos-root xfs        47G  3.6G   44G   8% /</span><br><span class="line">devtmpfs                devtmpfs  475M     0  475M   0% /dev</span><br><span class="line">tmpfs                   tmpfs     487M     0  487M   0% /dev/shm</span><br><span class="line">tmpfs                   tmpfs     487M   14M  473M   3% /run</span><br><span class="line">tmpfs                   tmpfs     487M     0  487M   0% /sys/fs/cgroup</span><br><span class="line">/dev/sda1               xfs      1014M  133M  882M  14% /boot</span><br><span class="line">tmpfs                   tmpfs      98M     0   98M   0% /run/user/0</span><br><span class="line">[root@lsy ~ 10:25:11]#</span><br></pre></td></tr></tbody></table></figure>

<p>2、停止所有节点的MDS（只有停掉MDS才可以删除文件系统）</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ~]# systemctl stop ceph-mds.target </span><br><span class="line"></span><br><span class="line">[root@ceph2 ~]# systemctl stop ceph-mds.target </span><br><span class="line"></span><br><span class="line">[root@ceph3 ~]# systemctl stop ceph-mds.target</span><br></pre></td></tr></tbody></table></figure>



<p>3、回到ceph1删除</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 10:26:48]# ceph  fs rm cephfs --yes-i-really-mean-it</span><br><span class="line">[root@ceph1 ceph 10:29:25]# ceph osd pool delete cephfs_metadata cephfs_metadata --yes-i-really-really-mean-it</span><br><span class="line">pool 'cephfs_metadata' removed</span><br><span class="line">[root@ceph1 ceph 10:30:04]# ecph osd pool delete cephfs_pool cephfs_pool --yes-i-really-really-mean-it</span><br><span class="line">bash: ecph: command not found</span><br><span class="line">[root@ceph1 ceph 10:30:43]# ceph osd pool delete cephfs_pool cephfs_pool --yes-i-really-really-mean-it</span><br><span class="line">pool 'cephfs_pool' removed</span><br></pre></td></tr></tbody></table></figure>

<p>此时查看装态为err</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 10:26:19]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     bb1d01eb-0d72-4794-9d7c-cb692d7a8f34</span><br><span class="line">    health: HEALTH_ERR</span><br><span class="line">            1 filesystem is degraded</span><br><span class="line">            1 filesystem has a failed mds daemon</span><br><span class="line">            1 filesystem is offline</span><br><span class="line">            insufficient standby MDS daemons available</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph1,ceph2,ceph3</span><br><span class="line">    mgr: ceph1(active), standbys: ceph2, ceph3</span><br><span class="line">    mds: cephfs-0/1/1 up , 1 failed</span><br><span class="line">    osd: 4 osds: 4 up, 4 in</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   2 pools, 192 pgs</span><br><span class="line">    objects: 22  objects, 3.1 KiB</span><br><span class="line">    usage:   4.0 GiB used, 16 GiB / 20 GiB avail</span><br><span class="line">    pgs:     192 active+clean</span><br></pre></td></tr></tbody></table></figure>

<p>4、再次启动mds服务</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 10:32:34]# systemctl start  ceph-mds.target</span><br></pre></td></tr></tbody></table></figure>

<p>此时查看状态，又回到健康状态</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 10:32:44]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     bb1d01eb-0d72-4794-9d7c-cb692d7a8f34</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"></span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph1,ceph2,ceph3</span><br><span class="line">    mgr: ceph1(active), standbys: ceph2, ceph3</span><br><span class="line">    osd: 4 osds: 4 up, 4 in</span><br><span class="line"></span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0  objects, 0 B</span><br><span class="line">    usage:   4.0 GiB used, 16 GiB / 20 GiB avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></tbody></table></figure>



<h5 id="创建ceph块存储"><a href="#创建ceph块存储" class="headerlink" title="创建ceph块存储"></a>创建ceph块存储</h5><h6 id="1、创建块存储并使用"><a href="#1、创建块存储并使用" class="headerlink" title="1、创建块存储并使用"></a>1、创建块存储并使用</h6><p>1、建立存储池并初始化</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 10:35:32]# ceph osd pool create rbd_pool 8</span><br><span class="line">pool 'rbd_pool' created</span><br><span class="line">[root@ceph1 ceph 10:35:47]# rbd pool init rbd_pool</span><br></pre></td></tr></tbody></table></figure>

<p>2、创建一个存储卷volume1，大小为5000M</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 10:37:06]# rbd  create volume1 --pool rbd_pool --size 5000</span><br><span class="line">[root@ceph1 ceph 10:37:35]# rbd ls rbd_pool</span><br><span class="line">volume1</span><br><span class="line">[root@ceph1 ceph 10:37:45]# rbd info volume1 -p rbd_pool</span><br><span class="line">rbd image 'volume1':</span><br><span class="line">        size 4.9 GiB in 1250 objects</span><br><span class="line">        order 22 (4 MiB objects)</span><br><span class="line">        id: 5ed46b8b4567</span><br><span class="line">        block_name_prefix: rbd_data.5ed46b8b4567</span><br><span class="line">        format: 2</span><br><span class="line">        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten</span><br><span class="line">        op_features:</span><br><span class="line">        flags:</span><br><span class="line">        create_timestamp: Tue Apr 21 10:37:35 2020</span><br><span class="line">[root@ceph1 ceph 10:38:03]#</span><br></pre></td></tr></tbody></table></figure>

<p>4、将创建的卷映射为块设备</p>
<p>因为rbd镜像的一些特性，OS kernel并不支持，所以映射报错</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 10:39:55]# rbd map rbd_pool/volume1</span><br><span class="line">rbd: sysfs write failed</span><br><span class="line">RBD image feature set mismatch. You can disable features unsupported by the kernel with "rbd feature disable rbd_pool/volume1 object-map fast-diff deep-flatten".</span><br><span class="line">In some cases useful info is found in syslog - try "dmesg | tail".</span><br><span class="line">rbd: map failed: (6) No such device or address</span><br><span class="line">[root@ceph1 ceph 10:40:10]#</span><br></pre></td></tr></tbody></table></figure>

<p>解决方法：distable相关特性</p>
<p>（四者之间有先后顺序问题）</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 10:44:36]#  rbd feature disable rbd_pool/volume1  fast-diff</span><br><span class="line">[root@ceph1 ceph 10:45:42]#  rbd feature disable rbd_pool/volume1  object-map</span><br><span class="line">[root@ceph1 ceph 10:45:45]#  rbd feature disable rbd_pool/volume1  exclusive-lock</span><br><span class="line">[root@ceph1 ceph 10:46:02]#</span><br><span class="line">[root@ceph1 ceph 10:48:17]#  rbd feature disable rbd_pool/volume1  deep-flatten</span><br></pre></td></tr></tbody></table></figure>

<p>再次映射</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 10:48:49]# rbd map rbd_pool/volume1</span><br><span class="line">/dev/rbd0</span><br></pre></td></tr></tbody></table></figure>



<p>5、查看映射（如果要取消映射，可以使用rbd unmap /dev/rbd0）</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 10:50:17]# rbd showmapped</span><br><span class="line">id pool     image   snap device</span><br><span class="line">0  rbd_pool volume1 -    /dev/rbd0</span><br><span class="line">[root@ceph1 ceph 10:50:28]#</span><br></pre></td></tr></tbody></table></figure>



<p>6、格式化挂载</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 10:50:51]# mkfs.xfs /dev/rbd0</span><br><span class="line">meta-data=/dev/rbd0              isize=512    agcount=8, agsize=160768 blks</span><br><span class="line">         =                       sectsz=512   attr=2, projid32bit=1</span><br><span class="line">         =                       crc=1        finobt=0, sparse=0</span><br><span class="line">data     =                       bsize=4096   blocks=1280000, imaxpct=25</span><br><span class="line">         =                       sunit=1024   swidth=1024 blks</span><br><span class="line">naming   =version 2              bsize=4096   ascii-ci=0 ftype=1</span><br><span class="line">log      =internal log           bsize=4096   blocks=2560, version=2</span><br><span class="line">         =                       sectsz=512   sunit=8 blks, lazy-count=1</span><br><span class="line">realtime =none                   extsz=4096   blocks=0, rtextents=0</span><br><span class="line">[root@ceph1 ceph 10:51:03]# mount /dev/rbd0  /mnt/</span><br><span class="line">df[root@ceph1 ceph 10:51:15]# df -Th |tail -1</span><br><span class="line">/dev/rbd0               xfs       4.9G   33M  4.9G   1% /mnt</span><br><span class="line">[root@ceph1 ceph 10:51:24]#</span><br></pre></td></tr></tbody></table></figure>





<p>补充：第二个客户端如果也要用此块存储，只需要执行以下几步就可以</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph2 yum.repos.d 10:52:45]# rbd  map rbd_pool/volume1</span><br><span class="line">/dev/rbd0</span><br><span class="line">[root@ceph2 yum.repos.d 10:53:03]# rbd showmapped</span><br><span class="line">id pool     image   snap device</span><br><span class="line">0  rbd_pool volume1 -    /dev/rbd0</span><br><span class="line">[root@ceph2 yum.repos.d 10:53:10]# mount /dev/rbd0 /mnt/</span><br><span class="line">[root@ceph2 yum.repos.d 10:53:24]# df -Th |tail -1</span><br><span class="line">/dev/rbd0               xfs       4.9G   33M  4.9G   1% /mnt</span><br><span class="line">[root@ceph2 yum.repos.d 10:53:31]#</span><br></pre></td></tr></tbody></table></figure>



<p>注意：块存储是不能实现同读和同写的，请不要两个客户端同时挂载进行读写。</p>
<h6 id="2、删除块存储方法"><a href="#2、删除块存储方法" class="headerlink" title="2、删除块存储方法"></a>2、删除块存储方法</h6><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph2 yum.repos.d 10:53:31]# umount /mnt/</span><br><span class="line">[root@ceph2 yum.repos.d 10:55:03]# rbd unmap /dev/rbd0</span><br><span class="line">[root@ceph2 yum.repos.d 10:55:15]# ceph osd pool delete rbd_pool rbd_pool --yes-i-really-really-mean-it</span><br><span class="line">pool 'rbd_pool' removed</span><br><span class="line">[root@ceph2 yum.repos.d 10:55:54]#</span><br></pre></td></tr></tbody></table></figure>

<h5 id="创建对象存储网关"><a href="#创建对象存储网关" class="headerlink" title="创建对象存储网关"></a>创建对象存储网关</h5><p>rgw(对象存储网关)：为客户端访问对象存储的接口</p>
<p>rgw的创建</p>
<p>1、在ceph集群任意一个节点上创建rgw(我这里为ceph1)</p>
<p>确保你已经装了这个软件包</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 11:11:37]# yum install -y ceph-radosgw</span><br></pre></td></tr></tbody></table></figure>

<p>创建rgw</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 11:11:37]# ceph-deploy rgw create ceph1</span><br></pre></td></tr></tbody></table></figure>

<p>2、验证7480端口</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph 11:11:25]# lsof -i:7480</span><br><span class="line">COMMAND    PID USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME</span><br><span class="line">radosgw 121388 ceph   41u  IPv4 1625237      0t0  TCP *:7480 (LISTEN)</span><br></pre></td></tr></tbody></table></figure>





<p>对象网关创建成功，后面的项目中我们会通过ceph1的对象网关来连接就可以使用了。</p>
<p>连接的地址为10.0.0.5:7480</p>
<h5 id="小结与说明："><a href="#小结与说明：" class="headerlink" title="小结与说明："></a>小结与说明：</h5><p>我们实现了ceph集群，并创建了三中类型存储，那么如何使用这些存储呢？</p>
<p>在不同的场景与应用中，会需要不同类型的存储类型，并不是说在一个项目里要把三中类型都用到</p>
<p>在后面的项目中，我们会分别用到这三种不同的类型。</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
    </section>
    <section class="extra">
      
        <section class="donate">
  <div class="qrcode">
    <img   class="lazyload" data-original="https://pic.izhaoo.com/alipay.jpg" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" >
  </div>
  <div class="icon">
    <a href="javascript:;" id="alipay"><i class="iconfont iconalipay"></i></a>
    <a href="javascript:;" id="wechat"><i class="iconfont iconwechat-fill"></i></a>
  </div>
</section>
      
      
      
<nav class="nav">
  
    <a href="/2020/05/18/%E4%BA%91%E8%AE%A1%E7%AE%97/"><i class="iconfont iconleft"></i>KVM虚拟化</a>
  
  
    <a href="/2020/05/02/hello-world/">Hello World<i class="iconfont iconright"></i></a>
  
</nav>

    </section>
    
  </section>
</article>
  </div>
</main>
  <footer class="footer">
  <div class="footer-social">
    
    
    
    
    
    <a href="tencent://message/?Menu=yes&uin=894519210 " target="_blank" onMouseOver="this.style.color= '#12B7F5'"
      onMouseOut="this.style.color='#33333D'">
      <i class="iconfont footer-social-item  iconQQ "></i>
    </a>
    
    
    
    
    
    <a href="javascript:; " target="_blank" onMouseOver="this.style.color= '#09BB07'"
      onMouseOut="this.style.color='#33333D'">
      <i class="iconfont footer-social-item  iconwechat-fill "></i>
    </a>
    
    
    
    
    
    <a href="https://www.instagram.com/izhaoo/ " target="_blank" onMouseOver="this.style.color= '#DA2E76'"
      onMouseOut="this.style.color='#33333D'">
      <i class="iconfont footer-social-item  iconinstagram "></i>
    </a>
    
    
    
    
    
    <a href="https://github.com/izhaoo " target="_blank" onMouseOver="this.style.color= '#24292E'"
      onMouseOut="this.style.color='#33333D'">
      <i class="iconfont footer-social-item  icongithub-fill "></i>
    </a>
    
    
    
    
    
    <a href="mailto:izhaoo@163.com " target="_blank" onMouseOver="this.style.color='#FFBE5B'"
      onMouseOut="this.style.color='#33333D'">
      <i class="iconfont footer-social-item  iconmail"></i>
    </a>
    
  </div>
  <div class="footer-copyright"><p>Copyright© 2019-2020 | <a target="_blank" href="https://liushiya-github.github.io/">刘世亚</a> .AllRightsReserved</p></div>
</footer>
  
      <div class="fab fab-plus">
    <i class="iconfont iconplus"></i>
  </div>
  <div class="fab fab-up">
    <i class="iconfont iconcaret-up"></i>
  </div>
  <div class="fab fab-menu">
    <i class="iconfont iconmenu"></i>
  </div>
  
</body>


<script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script>






<script src="https://cdn.bootcss.com/fancybox/3.5.7/jquery.fancybox.min.js"></script>






<script src="https://cdn.bootcss.com/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>






<script src="https://cdn.bootcdn.net/ajax/libs/jquery.lazyload/1.9.1/jquery.lazyload.min.js"></script>






<script src="https://cdn.bootcdn.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js"></script>




<script src="//cdn.jsdelivr.net/npm/leancloud-storage@4.1.0/dist/av-min.js"></script>
<script>
  AV.init({
    appId: '',
    appKey: '',
    serverURLs: '',
  });

  function showCount(Counter) {
    $(".leancloud").each(function (e) {
      (function (e) {
        var url = $(".leancloud").eq(e).attr('id').trim();
        var query = new AV.Query("Counter");
        query.equalTo("words", url);
        query.count().then(function (number) {
          $(".leancloud").eq(e).text(number ? number : '--');
        }, function (error) {});
      })(e)
    })
  }

  function addCount(Counter) {
    var url = $(".leancloud").length === 1 ? $(".leancloud").attr('id').trim() : 'https://LiuShiYa-github.github.io';
    var Counter = AV.Object.extend("Counter");
    var query = new Counter;
    query.save({
      words: url
    }).then(function (object) {})
  }
  
  $(function () {
    var Counter = AV.Object.extend("Counter");
    addCount(Counter);
    showCount(Counter);
  });
</script>



<script src="/js/script.js"></script>



<script>
  (function () {
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
      bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    } else {
      bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
  })();
</script>











</html>